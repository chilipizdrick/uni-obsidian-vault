{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 5. Предобработка данных и PCA\n",
    "## Ершов А.Г. КМБО-03-23, 2 семестр\n",
    "### Вариант 6\n",
    "Набор данных: [Denver Crime Data](https://www.kaggle.com/datasets/paultimothymooney/denver-crime-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Сколько в наборе данных объектов и признаков? Дать описание каждому признаку, если оно есть.\n",
    "\n",
    "Если отбросить всяческие ID, в наборе присутствует 16 признаков со следующими описаниями (нумерация идет по столбцам, столбцы, содержащие всяческие описания и ID - отброшены):\n",
    "\n",
    "3. `offense_code` - код преступления\n",
    "4. `offense_code_extension` - Расширение кода преступления (стоит \"склеить\" с `offence_code` или отбросить)\n",
    "7. `first_occurrence_date` - Когда происшествие произошло в первый раз\n",
    "8. `last_occurrence_date` - Когда происшествие произошло в последний раз относительно описываемого преступления\n",
    "9. `reported_date` - Когда о происшествии было доложено в полицию\n",
    "10. `incident_address` - Адрес улицы, на которой произошло происшествие\n",
    "11. `geo_x` - Гео-код по оси X\n",
    "12. `geo_y` - Гео-код по оси Y\n",
    "13. `geo_lat` - Широта, по которой произошло происшествие\n",
    "14. `geo_lon` - Долгота, по которой произошло происшествие\n",
    "15. `district_id` - округ\n",
    "16. `precinct_id` - участок\n",
    "17. `neighborhood_id` - район\n",
    "18. `is_crime` - 1, если преступление; 0, если нет\n",
    "19. `is_traffic` - 1, если дорожно транспортное происшествие 0, если нет\n",
    "20. `victim_count` - Число пострадавших в происшествии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Сколько категориальных признаков, какие?\n",
    "\n",
    "Из перечисленных в пункте 1 признаков категориальными являются \n",
    "\n",
    "3. `offense_code` - код преступления\n",
    "4. `offense_code_extension` - Расширение ID для преступления\n",
    "10. `incident_address` - Адрес, по которому произошло происшествие\n",
    "15. `district_id` - округ\n",
    "16. `precinct_id` - участок\n",
    "17. `neighborhood_id` - район"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Столбец с максимальным количеством уникальных значений категориального признака?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 386865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "incident_address          90518\n",
       "offense_code                141\n",
       "neighborhood_id              78\n",
       "precinct_id                  41\n",
       "district_id                   8\n",
       "offense_code_extension        7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./crime.csv', encoding='unicode_escape')\n",
    "features = ['offense_code', 'offense_code_extension', 'first_occurrence_date',\n",
    "            'last_occurrence_date', 'reported_date', 'incident_address',\n",
    "            'geo_x', 'geo_y', 'geo_lon', 'geo_lat', 'district_id',\n",
    "            'precinct_id', 'neighborhood_id', 'is_crime', 'is_traffic',\n",
    "            'victim_count']\n",
    "data = data.loc[:, data.columns.isin(features)]\n",
    "criterial_features = ['offense_code', 'offense_code_extension',\n",
    "                     'incident_address', 'district_id', 'precinct_id',\n",
    "                     'neighborhood_id']\n",
    "criterial_data = data.loc[:, data.columns.isin(criterial_features)]\n",
    "print(f'Number of rows: {len(data)}')\n",
    "criterial_data.nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Максимальное количество значений (90518) достигается у критериального признака `incident_addresses` (адрес, по которому произошло преступление), что не удивительно. Проводить классификацию по этому признаку, учитывая, что суммарное количество строк в датасете равно 386865 - бессмысленно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Есть ли бинарные признаки?\n",
    "\n",
    "Из перечисленных в пункте 1 признаков бинарными являются \n",
    "\n",
    "18. `is_crime` - 1, если преступление; 0, если нет\n",
    "19. `is_traffic` - 1, если дорожно транспортное происшествие 0, если нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Какие числовые признаки представлены?\n",
    "\n",
    "Из перечисленных в пункте 1 признаков числовыми являются \n",
    "\n",
    "7. `first_occurence_date` - Когда происшествие произошло в первый раз\n",
    "8. `last_occurrence_date` - Когда происшествие произошло в последний раз относительно описываемого преступления\n",
    "9. `reported_date` - Когда о происшествии было доложено в полицию\n",
    "11. `geo_x` - Гео-код по оси X\n",
    "12. `geo_y` - Гео-код по оси Y\n",
    "13. `geo_lat` - Широта, по которой произошло происшествие\n",
    "14. `geo_lon` - Долгота, по которой произошло происшествие\n",
    "20. `victim_count` - Число пострадавших в происшествии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Есть ли пропуски?\n",
    "\n",
    "Да, в датасете присутствуют пропуски.\n",
    "\n",
    "Количество пропусков в каждом из столбцов приведено ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Сколько объектов с пропусками?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of objects: 386865\n",
      "Number of objects with NA values: 180345\n",
      "Number of 'clean' objects: 206520\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of objects: {len(data)}')\n",
    "print(f'Number of objects with NA values: {len(data) - len(data.dropna())}')\n",
    "print(f'Number of \\'clean\\' objects: {len(data.dropna())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество объектов с пропусками оказалось равным 180345 (46.6% процентов от всех записей)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Столбец с максимальным количеством пропусков?\n",
    "\n",
    "Столбец с максимальным количеством пропусков - `last_occurence_date` (Когда происшествие произошло в последний раз относительно описываемого преступления).\n",
    "Также много пропусков в признаках, относящихся к местоположению происшествия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values per column:\n",
      "last_occurrence_date: 175556\n",
      "geo_lon: 15769\n",
      "geo_lat: 15769\n",
      "incident_address: 15503\n",
      "geo_x: 15503\n",
      "geo_y: 15503\n",
      "neighborhood_id: 689\n",
      "district_id: 57\n",
      "offense_code: 0\n",
      "offense_code_extension: 0\n",
      "first_occurrence_date: 0\n",
      "reported_date: 0\n",
      "precinct_id: 0\n",
      "is_crime: 0\n",
      "is_traffic: 0\n",
      "victim_count: 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of NA values per column:')\n",
    "for col in sorted(data.columns, \n",
    "                  key=lambda x: data[x].isna().sum(), \n",
    "                  reverse=True):\n",
    "    print(f'{col}: {data[col].isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Есть ли на ваш взгляд выбросы, аномальные значения?\n",
    "\n",
    "Чтобы проверить данные на выбросы, нормализуем их. Также отделим время происшествия от даты его происхождения в отдельный признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "numerical_features = ['first_occurrence_date', 'last_occurrence_date',\n",
    "                      'reported_date', 'geo_x', 'geo_y', 'geo_lat', 'geo_lon',\n",
    "                      'victim_count', 'reported_time', 'reported_time_utc']\n",
    "numerical_data = pd.DataFrame(index=data.index, columns=numerical_features)\n",
    "numerical_data.loc[:, numerical_features] = data.loc[:,\n",
    "    data.columns.isin(numerical_features)]\n",
    "\n",
    "time_features = ['first_occurrence_date',\n",
    "                 'last_occurrence_date', 'reported_date']\n",
    "\n",
    "for col in time_features:\n",
    "    numerical_data.loc[:, col] = numerical_data[col].apply(\n",
    "        lambda timestr: pd.to_datetime(timestr, format='%m/%d/%Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data['reported_time'] = \\\n",
    "    numerical_data['reported_date'].apply(lambda x: pd.to_datetime(\n",
    "        x.strftime('%H:%M:%S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data.loc[:, 'reported_time_utc'] = \\\n",
    "    numerical_data['reported_time'].dt.hour * 60 * 60 + \\\n",
    "    numerical_data['reported_time'].dt.minute * 60 + \\\n",
    "    numerical_data['reported_time'].dt.second\n",
    "numerical_data = numerical_data.astype('float64', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 15769\n",
      "Total number of objects: 386865\n"
     ]
    }
   ],
   "source": [
    "# Отбросим значения даты и времени в формате Timestamp для нормализации\n",
    "normalizible_features = ['geo_x', 'geo_y', 'geo_lat',\n",
    "                         'geo_lon', 'victim_count', 'reported_time_utc']\n",
    "normalizible_data = numerical_data.loc[:, numerical_data.columns.isin(\n",
    "    normalizible_features)]\n",
    "\n",
    "for feature in normalizible_features:\n",
    "    Q1 = normalizible_data[feature].quantile(0.25)\n",
    "    Q3 = normalizible_data[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    normalizible_data_no_outliers = normalizible_data[\n",
    "        (normalizible_data[feature] >=\n",
    "         lower_bound) & (normalizible_data[feature] <= upper_bound)]\n",
    "normalizible_data_no_outliers = normalizible_data_no_outliers.dropna()\n",
    "\n",
    "print(f\"Number of outliers: {len(normalizible_data) -\n",
    "                             len(normalizible_data_no_outliers)}\")\n",
    "print(f'Total number of objects: {len(normalizible_data)}')\n",
    "normalizible_data = normalizible_data_no_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Согласно алгоритму IQR, примененному на данных, приблизительно каждый 25-ый объект - выброс. При построении моделей и классификаторов исключим их из датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Столбец с максимальным средним значением после нормировки признаков через стандартное отклонение?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean values:\n",
      "geo_x: 5.650704189145687e-16\n",
      "geo_y: -9.493152402339494e-16\n",
      "geo_lat: 5.0441993575674176e-15\n",
      "geo_lon: 1.0637079181535481e-14\n",
      "victim_count: -3.342133424421861e-17\n",
      "reported_time_utc: 8.612383926124051e-17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_x</th>\n",
       "      <th>geo_y</th>\n",
       "      <th>geo_lat</th>\n",
       "      <th>geo_lon</th>\n",
       "      <th>victim_count</th>\n",
       "      <th>reported_time_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.043048</td>\n",
       "      <td>-0.014126</td>\n",
       "      <td>0.008177</td>\n",
       "      <td>-0.051917</td>\n",
       "      <td>-0.085388</td>\n",
       "      <td>-1.753158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039048</td>\n",
       "      <td>0.032791</td>\n",
       "      <td>0.038181</td>\n",
       "      <td>-0.046756</td>\n",
       "      <td>-0.085388</td>\n",
       "      <td>-2.157854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.062713</td>\n",
       "      <td>0.171868</td>\n",
       "      <td>0.127624</td>\n",
       "      <td>-0.076581</td>\n",
       "      <td>-0.085388</td>\n",
       "      <td>-1.537894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.062241</td>\n",
       "      <td>-0.085401</td>\n",
       "      <td>-0.037226</td>\n",
       "      <td>-0.076405</td>\n",
       "      <td>-0.085388</td>\n",
       "      <td>-0.599344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.068906</td>\n",
       "      <td>0.175905</td>\n",
       "      <td>0.128098</td>\n",
       "      <td>0.090668</td>\n",
       "      <td>-0.085388</td>\n",
       "      <td>-0.593604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      geo_x     geo_y   geo_lat   geo_lon  victim_count  reported_time_utc\n",
       "0 -0.043048 -0.014126  0.008177 -0.051917     -0.085388          -1.753158\n",
       "1 -0.039048  0.032791  0.038181 -0.046756     -0.085388          -2.157854\n",
       "2 -0.062713  0.171868  0.127624 -0.076581     -0.085388          -1.537894\n",
       "3 -0.062241 -0.085401 -0.037226 -0.076405     -0.085388          -0.599344\n",
       "4  0.068906  0.175905  0.128098  0.090668     -0.085388          -0.593604"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "normalized_numerical_data = pd.DataFrame(\n",
    "    std_scaler.fit_transform(normalizible_data.dropna()),\n",
    "    columns=normalizible_data.dropna().columns,\n",
    "    index=normalizible_data.dropna().index)\n",
    "print(\"Mean values:\")\n",
    "for col in normalizible_features:\n",
    "    print(f'{col}: {normalized_numerical_data[col].mean()}')\n",
    "normalized_numerical_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким столбцом оказался столбец `geo_lon`, однако все полученные средние значения очень близки к нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Столбец с целевым признаком?\n",
    "\n",
    "При выполнении задачи 6 целевыми признаками для соответствующих пунктов можно выделить:\n",
    "\n",
    "1. `district_id` - при выделении районов с наибольшим количеством преступлений\n",
    "2. `offence_code` - при выделении наиболее популярных преступлений в каждом районе и при построении предсказания типа преступления по времени и месту"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Сколько объектов попадает в тренировочную выборку при использовании train_test_split с параметрами test_size = 0.3, random_state = 42?\n",
    "\n",
    "Возьмем `offence_code` как целевой признак, и разделим датасет на тренировочную и целевую выборки с указанными параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "binary_features = ['is_crime', 'is_traffic']\n",
    "features = criterial_features + numerical_features + binary_features\n",
    "x_features = features.copy()\n",
    "x_features.remove('offense_code')\n",
    "x_features.remove('offense_code_extension')\n",
    "target = data['offense_code']\n",
    "x = data.loc[:, data.columns.isin(x_features)]\n",
    "\n",
    "x_tr, x_val, target_tr, target_val = train_test_split(\n",
    "    x, target, test_size=0.3, random_state=42)\n",
    "print(f'Training split size: {len(x_tr)}')\n",
    "print(f'Validation split size: {len(x_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тренировочную выборку попало 270805 объектов, в тестовую - 116060."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Между какими признаками наблюдается линейная зависимость (корреляция)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in normalizible_features:\n",
    "    print(normalized_numerical_data.loc[:].corr()[col][:], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наблюдается сильная лиейная зависимость между признаками:\n",
    "\n",
    "1. `first_occurrence_date` / `last_occurrence_date` и `reported_date`\n",
    "2. `geo_lat` и `geo_y`\n",
    "3. `geo_lon` и `geo_x`\n",
    "4. `reported_time` и `reported_time_utc`\n",
    "\n",
    "При дальнейшей обработке исключим некоторые из них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Сколько признаков достаточно для объяснения 90% дисперсии после применения метода PCA?\n",
    "\n",
    "Чтобы применить метод главных компонент, исключим мультиколлинеарные признаки, закодируем `district_id` через one-hot-encoding и объединим и закодируем `offense_code` и `offense_code_extension` через label-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_features = ['offence_code+extension', 'reported_time_utc',\n",
    "                  'geo_x', 'geo_y', 'district_id', 'is_crime',\n",
    "                  'is_traffic', 'victim_count']\n",
    "clean_data = pd.DataFrame(columns=clean_features, index=data.index)\n",
    "\n",
    "clean_data['offence_code+extension'] = data['offense_code'] * \\\n",
    "    10 + data['offense_code_extension']\n",
    "clean_data['reported_time_utc'] = numerical_data['reported_time_utc']\n",
    "clean_data['geo_x'] = numerical_data['geo_x']\n",
    "clean_data['geo_y'] = numerical_data['geo_y']\n",
    "clean_data['district_id'] = data['district_id']\n",
    "clean_data['is_crime'] = data['is_crime']\n",
    "clean_data['is_traffic'] = data['is_traffic']\n",
    "clean_data['victim_count'] = data['victim_count']\n",
    "clean_data = clean_data.dropna()\n",
    "# print(f'Number of objects in cleaned dataset: {len(clean_data.index)}')\n",
    "\n",
    "normalizible_clean_features = ['reported_time_utc', 'geo_x', 'geo_y',\n",
    "                               'victim_count']\n",
    "normalizible_clean_data = clean_data.loc[:, clean_data.columns.isin(\n",
    "    normalizible_clean_features)]\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "normalized_clean_data = pd.DataFrame(\n",
    "    std_scaler.fit_transform(normalizible_clean_data),\n",
    "    columns=normalizible_clean_data.columns,\n",
    "    index=normalizible_clean_data.index)\n",
    "\n",
    "for col in normalizible_clean_features:\n",
    "    clean_data[col] = normalized_clean_data.loc[:, col]\n",
    "clean_data = clean_data.apply(\n",
    "    lambda x: pd.to_numeric(x, errors='coerce')).dropna()\n",
    "\n",
    "clean_data.to_csv('./clean_crime.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca_data = clean_data.drop(columns=['offence_code+extension'])\n",
    "\n",
    "pca = PCA()\n",
    "x_pca = pca.fit(pca_data.values)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "n_components = np.argmax(cumulative_variance >= 0.9) + 1\n",
    "\n",
    "plt.barh(pca.get_feature_names_out(), explained_variance_ratio)\n",
    "plt.xlabel('Feature importances')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(range(1, len(cumulative_variance) + 1), cumulative_variance)\n",
    "# plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance)\n",
    "plt.xlabel('Cummulative variance by number of features')\n",
    "plt.show()\n",
    "\n",
    "print(f'{n_components} features are sufficient to explain 90% of the variance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, 4 признака достаточно для объяснения 90% дисперсии после применения метода PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Какой признак вносит наибольший вклад в первую компоненту?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pca.components_[0]\n",
    "feature_importances = pd.Series(\n",
    "    loadings, index=pca_data.columns)\n",
    "print('Feature contributions to the first component (pca0):')\n",
    "print(feature_importances.abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признак `district_id` вносит наибольший вклад в первую компоненту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Построить двухмерное представление данных с помощью алгоритма t-SNE. На сколько кластеров визуально, на ваш взгляд, разделяется выборка? Объяснить смысл кластеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "\n",
    "shuffled_data = shuffle(clean_data, random_state=0).head(20000)\n",
    "\n",
    "x = shuffled_data.drop(columns=['offence_code+extension',])\n",
    "y = shuffled_data.loc[:, clean_data.columns.isin(['offence_code+extension'])]\n",
    "\n",
    "z = TSNE(n_components=2, learning_rate='auto', random_state=0,\n",
    "         init='pca', perplexity=50).fit_transform(x)\n",
    "\n",
    "x_features = clean_features.copy()\n",
    "x_features.remove('offence_code+extension')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['y'] = y\n",
    "df['Component 1'] = z[:, 0]\n",
    "df['Component 2'] = z[:, 1]\n",
    "\n",
    "\n",
    "sns.scatterplot(x='Component 1', y='Component 2',\n",
    "                hue=shuffled_data['district_id'],\n",
    "                data=df).set(title='t-SNE Visualization (district_id)')\n",
    "plt.show()\n",
    "sns.scatterplot(x='Component 1', y='Component 2',\n",
    "                hue=shuffled_data['victim_count'],\n",
    "                data=df).set(title='t-SNE Visualization (victim_count)')\n",
    "plt.show()\n",
    "sns.scatterplot(x='Component 1', y='Component 2',\n",
    "                hue=shuffled_data['offence_code+extension'],\n",
    "                data=df).set(\n",
    "                    title='t-SNE Visualization (offence_code+extension)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На полученных графиках четко видно, что данные разделяются на 8 кластеров, 7 из которых представляют собой районы (`district_id`). Восьмой кластер (3-ий график сверху) - преступления c большим количесвом жертв, (их можно посмотреть в выводе кода ниже) которые имеют похожий код `131**` - различные виды массовой стрельбы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.sort_values(by=['victim_count'],\n",
    "                       ascending=False).loc[:,\n",
    "                        clean_data.columns.isin(['offence_code+extension', 'victim_count'])].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
